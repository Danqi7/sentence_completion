{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from utils import *\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('sentences count: ', 80208)\n",
      "('sentences[:2]', [u\"project gutenberg's etext of first book of adam and eve, by platt\\r\\npart one of a series of the forgotten books of eden\\r\\n\\r\\ncopyright laws are changing all over the world, be sure to check\\r\\nthe copyright laws for your country before posting these files!\", u'please take a look at the important information in this header.'])\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "file_directory = \"./data\"\n",
    "file_names = [join(file_directory, f) for f in listdir(file_directory) if isfile(join(file_directory, f)) \n",
    "                                                                          and f.endswith('.TXT')]\n",
    "\n",
    "sentences = []\n",
    "for file_name in file_names:\n",
    "    with open(file_name,'r') as f:\n",
    "        raw_data = f.read()\n",
    "        sentences += nltk.sent_tokenize(raw_data.decode('utf-8').lower())\n",
    "\n",
    "# Take a look at sentences in the data\n",
    "print (\"sentences count: \", len(sentences))\n",
    "print (\"sentences[:2]\", sentences[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 80208 sentences.\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 8000\n",
    "unknown_token = \"UNKOWN_TOKEN\"\n",
    "sentence_start_token =\"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "\n",
    "# Append SENTENCE_START and SENTENCE_END\n",
    "sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "print \"Parsed %d sentences.\" % (len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u\"SENTENCE_START project gutenberg's etext of first book of adam and eve, by platt\\r\\npart one of a series of the forgotten books of eden\\r\\n\\r\\ncopyright laws are changing all over the world, be sure to check\\r\\nthe copyright laws for your country before posting these files! SENTENCE_END\", u'SENTENCE_START please take a look at the important information in this header. SENTENCE_END', u'SENTENCE_START we encourage you to keep this file on your own disk, keeping an\\r\\nelectronic path open for the next readers. SENTENCE_END']\n"
     ]
    }
   ],
   "source": [
    "# Take a look at some parsed sentences\n",
    "print sentences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 51123 unique words tokens.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the sentences into words\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print \"Found %d unique words tokens.\" % len(word_freq.items())\n",
    "\n",
    "# Set the most common words and build index_to_word and word_to_index vectors\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w, i) for i, w in enumerate(index_to_word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_to_word[3]: SENTENCE_START\n",
      "word_to_index of previous one: 3\n",
      "Using vocabulary size 8000.\n",
      "The least frequent word in our vocabulary is 'blazing' and appeared 17 times.\n"
     ]
    }
   ],
   "source": [
    "# Take a look at some examples of index_to_word and word_to_index\n",
    "print \"index_to_word[3]: %s\" % index_to_word[3]\n",
    "print \"word_to_index of previous one: %d\" % word_to_index[index_to_word[3]]\n",
    "\n",
    "print \"Using vocabulary size %d.\" % vocabulary_size\n",
    "print \"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentence example:  [u'SENTENCE_START', u'project', u'gutenberg', u\"'s\", u'etext', u'of', u'first', u'book', u'of', u'adam', u'and', u'eve', u',', u'by', u'platt', u'part', u'one', u'of', u'a', u'series', u'of', u'the', u'forgotten', u'books', u'of', u'eden', u'copyright', u'laws', u'are', u'changing', u'all', u'over', u'the', u'world', u',', u'be', u'sure', u'to', u'check', u'the', u'copyright', u'laws', u'for', u'your', u'country', u'before', u'posting', u'these', u'files', u'!', u'SENTENCE_END']\n",
      "\n",
      "Example sentence: 'SENTENCE_START project gutenberg's etext of first book of adam and eve, by platt\n",
      "part one of a series of the forgotten books of eden\n",
      "\n",
      "copyright laws are changing all over the world, be sure to check\n",
      "the copyright laws for your country before posting these files! SENTENCE_END'\n",
      "\n",
      "Example sentence after Pre-processing: '[u'SENTENCE_START', u'project', u'gutenberg', u\"'s\", u'etext', u'of', u'first', u'book', u'of', u'adam', u'and', u'eve', u',', u'by', u'platt', u'part', u'one', u'of', u'a', u'series', u'of', u'the', u'forgotten', u'books', u'of', 'UNKOWN_TOKEN', u'copyright', u'laws', u'are', u'changing', u'all', u'over', u'the', u'world', u',', u'be', u'sure', u'to', u'check', u'the', u'copyright', u'laws', u'for', u'your', u'country', u'before', 'UNKOWN_TOKEN', u'these', u'files', u'!', u'SENTENCE_END']'\n"
     ]
    }
   ],
   "source": [
    "print \"Tokenized sentence example: \", tokenized_sentences[0] \n",
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "\n",
    "print \"\\nExample sentence: '%s'\" % sentences[0]\n",
    "print \"\\nExample sentence after Pre-processing: '%s'\" % tokenized_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "SENTENCE_START if our value per text is nominally estimated at one dollar then we produce $ 4 million dollars per hour this year as we release some eight text files per month : thus UNKOWN_TOKEN our UNKOWN_TOKEN from $ 2 million .\n",
      "[3, 51, 106, 1084, 1477, 2109, 18, 7739, 6925, 34, 50, 6015, 78, 55, 1106, 3438, 934, 2006, 2125, 1477, 615, 31, 362, 17, 55, 2036, 76, 1228, 2109, 2267, 1477, 1336, 40, 207, 7999, 106, 7999, 32, 3438, 635, 2006, 5]\n",
      "\n",
      "y:\n",
      "if our value per text is nominally estimated at one dollar then we produce $ 4 million dollars per hour this year as we release some eight text files per month : thus UNKOWN_TOKEN our UNKOWN_TOKEN from $ 2 million . SENTENCE_END\n",
      "[51, 106, 1084, 1477, 2109, 18, 7739, 6925, 34, 50, 6015, 78, 55, 1106, 3438, 934, 2006, 2125, 1477, 615, 31, 362, 17, 55, 2036, 76, 1228, 2109, 2267, 1477, 1336, 40, 207, 7999, 106, 7999, 32, 3438, 635, 2006, 5, 4]\n"
     ]
    }
   ],
   "source": [
    "# Create the training data\n",
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])\n",
    "\n",
    "# Print an training data example\n",
    "x_example, y_example = X_train[17], y_train[17]\n",
    "print \"x:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in x_example]), x_example)\n",
    "print \"\\ny:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in y_example]), y_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_train: ', (80208,))\n",
      "('y_train: ', (80208,))\n"
     ]
    }
   ],
   "source": [
    "# Print shape of X_train and y_train:\n",
    "print (\"X_train: \", X_train.shape)\n",
    "print (\"y_train: \", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4a477c2a3008>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Print\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"X_train[0].shape\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"X_train[1].shape\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"y_train[0].shape\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"y_train[0].shape\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# Print\n",
    "print (\"X_train[0].shape\", X_train[0].shape)\n",
    "print (\"X_train[1].shape\", X_train[1].shape)\n",
    "print (\"y_train[0].shape\", y_train[0].shape)\n",
    "print (\"y_train[0].shape\", y_train[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Batch Data Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Construct batch iterator\n",
    "# input data should be a numpy array of training sequences\n",
    "# Method next_batch returns the next batch tensor in shape [batch_size, max_time_steps] \n",
    "# next batch labels and sequence lengths list\n",
    "class PaddedDataIterator():\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.size = len(self.data)\n",
    "        self.cursor = 0\n",
    "    \n",
    "    def next_batch(self, batch_size):\n",
    "        if self.cursor + batch_size > len(self.data):\n",
    "            self.cursor = 0 # wrap around\n",
    "        batch_data = self.data[self.cursor:self.cursor+batch_size]\n",
    "        batch_labels = self.labels[self.cursor:self.cursor+batch_size]\n",
    "        self.cursor += batch_size\n",
    "        \n",
    "        lengths = [len(x) for x in batch_data]\n",
    "        max_length = max(lengths)\n",
    "        \n",
    "        # x in shape [batch_size, max_time_steps]\n",
    "        x = np.zeros([batch_size, max_length], np.float32)\n",
    "        # Zero pad the instances in batch so that their length equals to max_length\n",
    "        for i, x_at_i in enumerate(x):\n",
    "            x_at_i[:lengths[i]] = batch_data[i]\n",
    "        y = np.zeros([batch_size, max_length], np.float32)\n",
    "        # Zero pad the labels\n",
    "        for i, y_at_i in enumerate(y):\n",
    "            y_at_i[:lengths[i]] = batch_labels[i]\n",
    "            \n",
    "        # reshape the tensor into [batch_size, indivial time_steps]\n",
    "        x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "        \n",
    "        return x, y, lengths, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Input sequences\\n', <tf.Tensor 'Const_23:0' shape=(5, 50) dtype=float32>)\n",
      "('Input sequences labels\\n', array([[  4.07000000e+02,   8.73000000e+02,   4.80000000e+01,\n",
      "          4.32000000e+02,   2.00000000e+00,   1.16000000e+02,\n",
      "          4.82000000e+02,   2.00000000e+00,   2.88000000e+02,\n",
      "          6.00000000e+00,   4.79000000e+02,   0.00000000e+00,\n",
      "          2.20000000e+01,   6.28400000e+03,   1.67000000e+02,\n",
      "          5.00000000e+01,   2.00000000e+00,   8.00000000e+00,\n",
      "          2.46900000e+03,   2.00000000e+00,   1.00000000e+00,\n",
      "          1.97200000e+03,   6.62000000e+02,   2.00000000e+00,\n",
      "          7.99900000e+03,   1.66400000e+03,   2.22000000e+02,\n",
      "          4.20000000e+01,   2.89100000e+03,   3.70000000e+01,\n",
      "          1.22000000e+02,   1.00000000e+00,   1.56000000e+02,\n",
      "          0.00000000e+00,   2.80000000e+01,   7.51000000e+02,\n",
      "          7.00000000e+00,   2.22200000e+03,   1.00000000e+00,\n",
      "          1.66400000e+03,   2.22000000e+02,   2.10000000e+01,\n",
      "          9.20000000e+01,   1.66000000e+02,   1.23000000e+02,\n",
      "          7.99900000e+03,   7.90000000e+01,   2.26700000e+03,\n",
      "          7.20000000e+01,   4.00000000e+00],\n",
      "       [  9.55000000e+02,   1.58000000e+02,   8.00000000e+00,\n",
      "          2.80000000e+02,   3.40000000e+01,   1.00000000e+00,\n",
      "          4.62000000e+02,   1.07800000e+03,   9.00000000e+00,\n",
      "          3.10000000e+01,   7.99900000e+03,   5.00000000e+00,\n",
      "          4.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00],\n",
      "       [  5.50000000e+01,   3.57100000e+03,   4.30000000e+01,\n",
      "          7.00000000e+00,   4.35000000e+02,   3.10000000e+01,\n",
      "          2.90600000e+03,   3.30000000e+01,   9.20000000e+01,\n",
      "          1.07000000e+02,   3.27500000e+03,   0.00000000e+00,\n",
      "          1.90300000e+03,   4.60000000e+01,   5.19300000e+03,\n",
      "          1.24500000e+03,   4.41000000e+02,   2.10000000e+01,\n",
      "          1.00000000e+00,   4.50000000e+02,   1.54600000e+03,\n",
      "          5.00000000e+00,   4.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00],\n",
      "       [  8.30000000e+01,   2.60000000e+01,   1.67400000e+03,\n",
      "          3.10000000e+01,   5.00000000e+00,   4.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00],\n",
      "       [  7.99900000e+03,   7.00000000e+00,   1.00000000e+00,\n",
      "          1.56000000e+02,   2.00000000e+00,   3.26000000e+02,\n",
      "          7.17000000e+02,   7.99900000e+03,   5.19300000e+03,\n",
      "          7.99900000e+03,   7.99900000e+03,   4.10100000e+03,\n",
      "          2.20000000e+01,   2.43000000e+02,   7.99900000e+03,\n",
      "          6.00000000e+00,   2.20000000e+01,   7.99900000e+03,\n",
      "          0.00000000e+00,   2.71000000e+02,   7.99900000e+03,\n",
      "          7.99900000e+03,   1.22600000e+03,   8.34000000e+02,\n",
      "          2.20000000e+01,   4.45000000e+03,   2.00000000e+00,\n",
      "          4.86000000e+03,   6.00000000e+00,   7.99900000e+03,\n",
      "          1.07800000e+03,   3.30000000e+01,   7.99900000e+03,\n",
      "          4.07000000e+02,   8.73000000e+02,   7.00000000e+00,\n",
      "          2.50000000e+02,   1.22600000e+03,   0.00000000e+00,\n",
      "          6.00000000e+00,   8.36000000e+02,   1.07800000e+03,\n",
      "          1.80000000e+01,   3.85600000e+03,   1.06700000e+03,\n",
      "          5.00000000e+00,   4.00000000e+00,   0.00000000e+00,\n",
      "          0.00000000e+00,   0.00000000e+00]], dtype=float32), (5, 50))\n",
      "('Input sequences lengths\\n', [50, 13, 23, 6, 47])\n"
     ]
    }
   ],
   "source": [
    "data_it = PaddedDataIterator(X_train, y_train)\n",
    "d = data_it.next_batch(5)\n",
    "print('Input sequences\\n', d[0])\n",
    "print('Input sequences labels\\n', d[1], d[1].shape)\n",
    "print('Input sequences lengths\\n', d[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Input sequences\\n', <tf.Tensor 'Const_24:0' shape=(5, 52) dtype=float32>)\n",
      "('Input sequences labels[0]\\n', (5, 52))\n",
      "('Input sequences lengths\\n', [6, 52, 12, 26, 26], 52)\n"
     ]
    }
   ],
   "source": [
    "# Take another look at the iterator\n",
    "d2 = data_it.next_batch(5)\n",
    "print('Input sequences\\n', d2[0])\n",
    "print('Input sequences labels[0]\\n', d2[1].shape)\n",
    "print('Input sequences lengths\\n', d2[2], d2[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Graph for dynamic LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    \n",
    "def build_LSTM_graph(vocab_size, num_classes, state_size = 1000, batch_size = 256):\n",
    "    '''\n",
    "    Build LSMT graph\n",
    "    Return unscaled logits in shape [batch_size, time_steps, num_classes]\n",
    "    '''\n",
    "    reset_graph()\n",
    "    # Placeholders\n",
    "    x = tf.placeholder(tf.int32, [batch_size, None]) # [batch_size, num_steps]\n",
    "    y = tf.placeholder(tf.int32, [batch_size])\n",
    "    seqlen = tf.placeholder(tf.int32, [batch_size])\n",
    "    max_len = tf.placeholder(tf.int32, [])\n",
    "    \n",
    "    # Parameters\n",
    "    # Wout: [state_size, num_classes]\n",
    "    # b: [num_classes]\n",
    "    weights = {\n",
    "        'out': tf.Variable(tf.random_normal([state_size, num_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # Embedding layer\n",
    "    embeddings = tf.get_variable('embedding_matrix', [vocab_size, state_size])\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "    \n",
    "    # LSTM\n",
    "    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(state_size, forget_bias=1.0)\n",
    "    outputs, state = tf.nn.dynamic_rnn(cell = lstm_cell, \n",
    "                                       inputs = rnn_inputs, \n",
    "                                       sequence_length=seqlen,\n",
    "                                       dtype=tf.float32)\n",
    "    \n",
    "    # Softmax layer\n",
    "    # outputs in shape [batch_size, time_steps, state_size]\n",
    "    # y in shape [batch_size, time_steps]\n",
    "    outputs = tf.reshape(outputs, [-1, state_size]) # (B*T, state_size)\n",
    "    y = tf.reshape(y, [-1]) # y in shape (B*T)\n",
    "    logits = tf.matmul(outputs, weights['out']) + biases['out']\n",
    "#     logits = tf.reshape(logits, [batch_size, -1, num_classes]) # (B, T, num_classes)\n",
    "    \n",
    "    # Unstack outputs to get a list of \"time_steps\" of (batch_size, state_size)\n",
    "#     outputs_list = tf.unstack(outputs, max_len, axis=1)\n",
    "#     logits_list = [] # [time_steps, batch_size, num_classes]\n",
    "#     for output in outputs_list:\n",
    "#         logits_list.append(tf.matmul(output, weights['out']) + biases['out']) # (batch_size, num_classes)\n",
    "#     logits = tf.transpose(logits_list, (1, 0, 2)) # now logits are in shape [batch_size, time_steps, num_classes]\n",
    "    \n",
    "    # Create a mask to remove extra costs caused by paddings\n",
    "    weights = tf.sequence_mask(seqlen, max_len, dtype=tf.float32) # weights in shape of [batch_size, time_steps]\n",
    "    return {\n",
    "        'logits': logits, # (B*T, num_classes)\n",
    "        'y': y,\n",
    "        'weights': weights,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "\n",
    "# vocabulary_size = 8000\n",
    "state_size = 1000\n",
    "num_classes = vocabulary_size\n",
    "\n",
    "# Learning Parameters\n",
    "learning_rate = 0.01\n",
    "training_steps = 10000\n",
    "batch_size = 256\n",
    "display_step = 1000\n",
    "\n",
    "g = build_LSTM_graph(vocabulary_size, num_classes, state_size, batch_size)\n",
    "loss = tf.contrib.seq2seq.sequence_loss(\n",
    "    tf.expand_dims(g['logits'], 0), # (1, B*T, num_classes)\n",
    "    tf.expand_dims(g['y'], 0), # (1, B*T)\n",
    "    g['weights'],\n",
    "    average_across_timesteps=False,\n",
    "    average_across_batch=True)\n",
    "#total_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=g['logits'], labels=g['y']) # y :[B*T]\n",
    "\n",
    "# Mask out the padding\n",
    "#total_loss = \n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "# Evaluate model\n",
    "# correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"Placeholder:0\", shape=(256, ?), dtype=int32) is not an element of this graph.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-6425fd044314>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_seqlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_itr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# Run optimization op (backprop)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         sess.run(optimizer, feed_dict={x: batch_x\n\u001b[0m\u001b[1;32m     12\u001b[0m                                        })\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdisplay_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/danqiliao/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/danqiliao/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1065\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m             raise TypeError('Cannot interpret feed_dict key as Tensor: '\n\u001b[0;32m-> 1067\u001b[0;31m                             + e.args[0])\n\u001b[0m\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"Placeholder:0\", shape=(256, ?), dtype=int32) is not an element of this graph."
     ]
    }
   ],
   "source": [
    "data_itr = PaddedDataIterator(X_train, y_train)\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, training_steps+1):\n",
    "        batch_x, batch_y, batch_seqlen, max_len = data_itr.next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x\n",
    "                                       })\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch accuracy & loss\n",
    "            loss = sess.run([accuracy, loss], feed_dict={x: batch_x, y: batch_y,\n",
    "                                                seqlen: batch_seqlen})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy\n",
    "#     test_data = testset.data\n",
    "#     test_label = testset.labels\n",
    "#     test_seqlen = testset.seqlen\n",
    "#     print(\"Testing Accuracy:\", \\\n",
    "#         sess.run(accuracy, feed_dict={x: test_data, y: test_label,\n",
    "#                                       seqlen: test_seqlen}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
